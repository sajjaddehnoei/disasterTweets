---
title: "Untitled"
author: "Sajjad"
date: "3/21/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(tidyverse)
library(stringr)
library(lubridate)
library(tidytext)
library(tm)
library(wordcloud)
library(wordcloud2)
library(irlba)
library(gridExtra)
library(qdap)
library(keras)
```

```{r readData}
train <- read_csv("train.csv")
test <- read_csv("test.csv")
```

# First, I'm Combining the two datasets to perform some cleaning. 
* note: technically, you're not supposed to do any processing before splitting  data.
```{r rBind}
x <- rbind(train%>%select(-target), test)
```

```{r cleaning__1}
x <- x %>%
  mutate(text = str_to_lower(text),
         text = gsub("http[^[:space:]]*", "", text),
         youTube = if_else(str_detect(text, "youtube"), 1, 0),
         mentions = if_else(str_detect(text, "@"), 1, 0),
         text = stemDocument(text, language = "english"), 
         text = removePunctuation(text),
         text = str_trim(stripWhitespace(text)))
```

```{r countwords}
x <- x %>%
  mutate(numStrings = str_count(text), 
         numWords = str_count(text, '\\w+'))
```

```{r removeEmojies}
emojis <- c("\U0001F600-\U0001F64F", "\U0001F300-\U0001F5FF", "\U0001F680-\U0001F6FF", "\U0001F1E0-\U0001F1FF", "\U00002702-\U000027B0", "\U000024C2-\U0001F251")

for (i in length(emojis)) {
 x <- x %>%
  mutate(text = gsub(emojis[i], "", text, perl = T)) 
}
```

```{r removeStopWords}
x <- unnest_tokens(x, word, text) %>%
  anti_join(stop_words, by=c("word" = "word"))
```

```{r cleaning__2}
x <- x %>%
  group_by(id) %>%
  mutate(sentence = paste0(word, collapse =" ")) %>%
  mutate(keyword = if_else(word %in% unique(x$keyword) & is.na(keyword), 
                           word, keyword))
```

```{r cleaning__3}
x <- x %>%
  left_join(get_sentiments(), by= c("word" = "word"))
```

```{r trainSwitch}
xTrain <- x %>%
  filter(!duplicated(id)) %>%
  select(id, youTube, mentions, numStrings, numWords, sentence) %>%
  right_join(train %>% select(id, keyword, location, target), by = "id") %>%
  mutate(location = as.factor(location), keyword = as.factor(keyword)) %>%
  rename(text = sentence) %>%
  filter(!is.na(text))
```

```{r testSwitch}
xTest <- x %>%
  filter(!duplicated(id)) %>%
  select(id, youTube, mentions, numStrings, numWords, sentence) %>%
  right_join(test %>% select(id, keyword, location), by = "id") %>%
  mutate(location = as.factor(location), keyword = as.factor(keyword)) %>%
  rename(text = sentence) %>%
  filter(!is.na(text))
```

```{r desc__1}
# df <- x %>%
#   group_by(keyword) %>%
#   summarise(n=n(), one = sum(target == 1), zero = sum(target == 0), 
#             disasterRatio = one / (one + zero)) %>%
#   arrange(desc(disasterRatio))
df <- x %>%
  group_by(keyword) %>%
  summarise(n=n()) %>%
  arrange(n)


# wordcloud(words = df$keyword, freq = df$n, min.freq = 3, random.order = F, 
#           colors = brewer.pal(8, "Dark2"), max.words = 10)
wordcloud2(df)
```

```{r desc__2}
df <- xTrain %>%
  group_by(location) %>%
  summarise(n=n(), one = sum(target == 1), zero = sum(target == 0), 
            disasterRatio = one / (one + zero)) %>%
  arrange(desc(disasterRatio))
```

```{r desc__3}
df <- x %>%
  group_by(target) %>%
  summarise(numStrings = mean(numStrings), numWords = mean(numWords))
```

```{r desc__4}
df <- x %>%
  group_by(sentiment) %>%
  summarise(n=n(), one = sum(target == 1), zero = sum(target == 0), 
            disasterRatio = one / (one + zero)) %>%
  arrange(desc(disasterRatio))
```

# Building the model

```{r classNames}
classNames <- c(1, 0)
```

```{r textVectorizer}
num_words <- nrow(x %>% filter(!duplicated(word))) + 1
max_length <- 140 
# 140 is the maximum length for a tweet when the data was collected
text_vectorization <- 
  layer_text_vectorization(max_tokens = num_words, 
                           output_sequence_length = max_length)

text_vectorization %>% adapt(xTrain$text)

get_vocabulary(text_vectorization)
text_vectorization(matrix(xTrain$text[1], ncol = 1))
```

```{r buildModel}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(nrow(x %>% filter(!duplicated(word))) + 1, 
                  output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu", 
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 16, activation = "relu", 
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model <- keras_model(input, output)
```

```{r compileModel}
model %>% compile(
  optimizer = 'adam',
  loss = 'binary_crossentropy',
  metrics = list('accuracy')
)
```


```{r trainModel}
history <- model %>% 
  keras::fit(
  xTrain$text,
  xTrain$target,
  epochs = 10,
  batch_size = 250,
  validation_split = 0.2,
  verbose=2)
```


